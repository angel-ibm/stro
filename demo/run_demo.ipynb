{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO  \n",
    "## Anomaly dection using Milvus, Presto and Kafka on a Lakehouse\n",
    "\n",
    "![Top](../images/cover1.png \"cover1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install pre-requisites\n",
    "\n",
    "The pre-requisites should have been already installed during the setup process but we can run it again to be sure. No worries if you see many messages like \"Requirement already satisfied\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%system python3 -m pip install pymilvus confluent-kafka astropy scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialize Milvus with vectorized images to create a private repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section loads sky pictures into a Milvus collection. The images were taken in FITS format and contain the metadata in the header of the file. The collection is built with the original file name, the vector corresponding to the image and a few trailing fields as metadata. Actually, it would have not been necessary to store the metadata in the collection because it is suppoused to be managed separately in a Iceberg / Presto / Lakehouse environment. However, for demo purposes it is handy to include it in Milvus for easy experimentation.\n",
    "\n",
    "You need to run this section only once or if you want to re-create the full collection from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to extract a certificate that we will use for authentication. As it will not change during the life of the environment, it is only necessary to run this cell the first time the demo is run and never again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = %system echo QUIT | openssl s_client -showcerts -connect watsonxdata:8443 | \\\n",
    "        awk '/-----BEGIN CERTIFICATE-----/ {p=1}; p; /-----END CERTIFICATE-----/ {p=0}' > ./presto.crt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a few functions are implemented. They will be called during the main workflow of the section. This cell will trigger no action. It is only definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from pymilvus import(\n",
    "    Milvus,\n",
    "    IndexType,\n",
    "    Status,\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    utility,\n",
    "    MilvusClient\n",
    ")\n",
    "\n",
    "from astropy.io import fits\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "# Connects to Milvus database using the provided host, port, user, password, \n",
    "# and server certificate path.\n",
    "def connect_to_milvus() :\n",
    "    \n",
    "    host         = 'watsonxdata'\n",
    "    port         = 19530\n",
    "    user         = 'ibmlhadmin'        \n",
    "    key          = 'password'\n",
    "    server_pem_path = 'presto.crt'\n",
    "    connections.connect(alias='default',\n",
    "                       host=host,\n",
    "                       port=port,\n",
    "                       user=user,\n",
    "                       password=key,\n",
    "                       server_pem_path=server_pem_path,\n",
    "                       server_name='watsonxdata',\n",
    "                       secure=True)  \n",
    " \n",
    "\n",
    "# Creates a collection for storing image embeddings. \n",
    "# It is created with an IVF_FLAT index on the \"embedding\" field.\n",
    "# Returns he created collection object\n",
    "def create_collection():\n",
    "    \n",
    "    utility.drop_collection(\"image_embeddings\")\n",
    "    \n",
    "    fields = [\n",
    "        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=\"file_path\", dtype=DataType.VARCHAR, max_length=128),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=16600),\n",
    "        FieldSchema(name=\"image_width\", dtype=DataType.VARCHAR, max_length=128),\n",
    "        FieldSchema(name=\"image_height\", dtype=DataType.VARCHAR, max_length=128),\n",
    "        FieldSchema(name=\"image_utz\", dtype=DataType.VARCHAR, max_length=128),\n",
    "        FieldSchema(name=\"object_name\", dtype=DataType.VARCHAR, max_length=128),\n",
    "        FieldSchema(name=\"object_ra\", dtype=DataType.VARCHAR, max_length=128),\n",
    "        FieldSchema(name=\"object_dec\", dtype=DataType.VARCHAR, max_length=128),\n",
    "        FieldSchema(name=\"object_alt\", dtype=DataType.VARCHAR, max_length=128),\n",
    "        FieldSchema(name=\"object_az\", dtype=DataType.VARCHAR, max_length=128),\n",
    "        FieldSchema(name=\"camera_focus\", dtype=DataType.VARCHAR, max_length=128),\n",
    "        FieldSchema(name=\"local_temp\", dtype=DataType.VARCHAR, max_length=128),\n",
    "        FieldSchema(name=\"local_lat\", dtype=DataType.VARCHAR, max_length=128),\n",
    "        FieldSchema(name=\"local_long\", dtype=DataType.VARCHAR, max_length=128),\n",
    "        FieldSchema(name=\"local_weather\", dtype=DataType.VARCHAR, max_length=128)\n",
    "\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, \"Embedding of FITS image file\")\n",
    "    \n",
    "    fits_coll = Collection(\"image_embeddings\", schema)\n",
    "\n",
    "    index_params = {\n",
    "            'metric_type':'L2',\n",
    "            'index_type':\"IVF_FLAT\",\n",
    "            'params':{\"nlist\":2048}\n",
    "    }\n",
    "    fits_coll.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "\n",
    "    fits_coll.flush()\n",
    "    \n",
    "    return(fits_coll)\n",
    "\n",
    "# Load a FITS file and return the header and resized data\n",
    "# of the image file passed as parameter\n",
    "def load_fits_file(file_path) :\n",
    "    \n",
    "    with fits.open(file_path) as hdul:\n",
    "        image_header = hdul[0].header\n",
    "        image_data = hdul[0].data\n",
    "        image_resized = resize(image_data, (166, 100), mode='reflect')\n",
    "        # The resize function resizes the image data to the \n",
    "        # dimensions (166, 100) (166 pixels wide and 100 pixels high).\n",
    "        # The mode='reflect' parameter handles how the image is resized by \n",
    "        # reflecting the edge values beyond the image boundaries when \n",
    "        # resizing, which prevents boundary artifacts.\n",
    "\n",
    "    return (image_header,image_resized) \n",
    "\n",
    "# Generates an embedding vector from an image\n",
    "# and resturns it as a NumPy array\n",
    "def generate_embedding(image_data) : \n",
    "    \n",
    "    embedding = image_data.flatten() # The flatten() method converts the multi-dimensional \n",
    "                                     # FITS image data (which could be 2D or 3D) into a \n",
    "                                     # one-dimensional array (vector). This process simplifies \n",
    "                                     # the data into a long list of pixel values\n",
    "    embedding = embedding / np.linalg.norm(embedding)   # The vector is then normalized by \n",
    "                                                        # dividing it by its L2 norm \n",
    "                                                        # (Euclidean norm). The L2 norm is \n",
    "                                                        # the square root of the sum of the \n",
    "                                                        # squares of the values in the array. \n",
    "                                                        # This operation scales the vector so \n",
    "                                                        # that its magnitude becomes 1, turning\n",
    "                                                        #  it into a unit vector.\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Inserts an embedding into the Milvus collection\n",
    "def insert_embedding(fits_coll, file_path, header, embedding):\n",
    "\n",
    "    # To simplify things, all fields are strings\n",
    "    image_width = str(header['NAXIS1'])\n",
    "    image_height = str(header['NAXIS2'])\n",
    "    image_utz = header['UT-OBS']\n",
    "    object_name = header['OBJECT']\n",
    "    object_ra = str(header['RA'])\n",
    "    object_dec = str(header['DEC'])\n",
    "    object_alt =str( header['TELALT'])\n",
    "    object_az = str(header['TELAZ'])\n",
    "    camera_focus = str(header['CAMFOCUS'])\n",
    "    local_temp = str(header['TELTEMP'])\n",
    "    local_lat = str(header['LATITUDE'])\n",
    "    local_long = str(header['LONGITUD'])\n",
    "    local_weather = str(header['WEATHER'])\n",
    "\n",
    "    # Note the multiple brackets. It is the syntax imposed by the method\n",
    "    fits_coll.insert([  [file_path], \n",
    "                        [embedding],\n",
    "                        [image_width], \n",
    "                        [image_height], \n",
    "                        [image_utz], \n",
    "                        [object_name], \n",
    "                        [object_ra], \n",
    "                        [object_dec], \n",
    "                        [object_alt],\n",
    "                        [object_az], \n",
    "                        [camera_focus], \n",
    "                        [local_temp], \n",
    "                        [local_lat], \n",
    "                        [local_long], \n",
    "                        [local_weather]   \n",
    "                      ])\n",
    "    fits_coll.load()\n",
    "\n",
    "# Takes a bunch of images in FITS format and inserts them\n",
    "# in a Milvus collection, generating vectors for the graphical content\n",
    "def initialize_collection():\n",
    "    fits_coll = create_collection()\n",
    "    file_paths = glob.glob(\"./images/m31*.FITS\")\n",
    "    for image_file in sorted(file_paths):\n",
    "        print(\"Inserting file: \", image_file)\n",
    "        image_header, image_data = load_fits_file(image_file)\n",
    "        embedding_vector = generate_embedding(image_data)\n",
    "        insert_embedding(fits_coll, image_file, image_header, embedding_vector)\n",
    "    return fits_coll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell triggers the initalization of the Milvus collection using the functions defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_to_milvus()\n",
    "fits_coll = initialize_collection()\n",
    "connections.disconnect(alias=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize Metadata in watsonx.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is intended to illustrate how to extract the metadata of a set of images in FITS format and store it in a relational table using the Presto and Iceberg technology contained in watsonx.data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, a few functions are implemented. They will be called during the main workflow of the section. This cell will trigger no action. It is only definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import prestodb\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from astropy.io import fits\n",
    "\n",
    "# Connect to watsonx.data using the Presto engine.\n",
    "# Note that this function is intended to run on the same system\n",
    "# where watsonx.data is installed. If you want to execute the code\n",
    "# directly on your laptop, the address, the port and the certificate\n",
    "# must be updated \n",
    "def connect_to_watsonxdata() :\n",
    "\n",
    "    # Connection Parameters\n",
    "    userid     = 'ibmlhadmin'\n",
    "    password   = 'password'\n",
    "    hostname   = 'watsonxdata'\n",
    "    port       = '8443'\n",
    "    catalog    = 'tpch'\n",
    "    schema     = 'tiny'\n",
    "    certfile   = \"/certs/lh-ssl-ts.crt\"\n",
    "\n",
    "    # Connect Statement\n",
    "    try:\n",
    "        wxdconnection = prestodb.dbapi.connect(\n",
    "                host=hostname,\n",
    "                port=port,\n",
    "                user=userid,\n",
    "                catalog=catalog,\n",
    "                schema=schema,\n",
    "                http_scheme='https',\n",
    "                auth=prestodb.auth.BasicAuthentication(userid, password)\n",
    "        )\n",
    "        if (certfile != None):\n",
    "            wxdconnection._http_session.verify = certfile\n",
    "        print(\"Connection successful\")\n",
    "        return wxdconnection\n",
    "    except Exception as e:\n",
    "        print(\"Unable to connect to the database.\")\n",
    "        print(repr(e))\n",
    "\n",
    "# Creates an empty table to hold the metadata of the FITS image files\n",
    "# For illustration, experimentation and demo purposes an extra field has \n",
    "# been added (filebytes) to hold the graphical contents too. \n",
    "def create_metadata_table(wxdconnection) :\n",
    "\n",
    "    cursor = wxdconnection.cursor()\n",
    "\n",
    "    sql = '''\n",
    "        CREATE SCHEMA IF NOT EXISTS \n",
    "            iceberg_data.fits \n",
    "        WITH (location = 's3a://iceberg-bucket/fits') \n",
    "    '''\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "    except Exception as err:\n",
    "        print(repr(err))\n",
    "    \n",
    "    sql = '''\n",
    "        DROP TABLE IF EXISTS \n",
    "            iceberg_data.fits.\"fits-images\"\n",
    "    '''\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "    except Exception as err:\n",
    "        print(repr(err))\n",
    "        \n",
    "    sql = '''\n",
    "         CREATE TABLE \n",
    "            iceberg_data.fits.\"fits-images\" (\n",
    "                filename   VARCHAR,\n",
    "                filebytes  VARCHAR,\n",
    "                image_width VARCHAR,\n",
    "                image_height VARCHAR,\n",
    "                image_utz VARCHAR,\n",
    "                object_name VARCHAR,\n",
    "                object_ra VARCHAR,\n",
    "                object_dec VARCHAR,\n",
    "                object_alt VARCHAR,\n",
    "                object_az VARCHAR,\n",
    "                camera_focus VARCHAR,\n",
    "                local_temp VARCHAR,\n",
    "                local_lat VARCHAR,\n",
    "                local_long VARCHAR,\n",
    "                local_weather VARCHAR\n",
    "            )\n",
    "    '''\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "    except Exception as err:\n",
    "        print(repr(err))\n",
    "\n",
    "    cursor.close()\n",
    "\n",
    "# Extract the metadata of a FITS file and store it into the table\n",
    "# created in the previous function. \n",
    "# For illustration, experimentation and demo purposes an extra field has \n",
    "# been added (filebytes) to hold the graphical contents too. Note how there\n",
    "# is no embedding or vector in this field, only the contents of the file.\n",
    "# However, the file has been encoded with base64 in ortder to avoid potential \n",
    "# problems with control characters, escape sequences and other things that\n",
    "# may cause and strange behaviuor.\n",
    "def insert_file(wxdconnection, image_file):\n",
    "\n",
    "    with open(image_file, 'rb') as file:\n",
    "        file_content = file.read()\n",
    "\n",
    "    encoded_file_content = base64.b64encode(file_content).decode('utf-8')\n",
    "\n",
    "    with fits.open(image_file) as hdul:\n",
    "        image_header = hdul[0].header\n",
    "\n",
    "        image_width = str(image_header['NAXIS1'])\n",
    "        image_height = str(image_header['NAXIS2'])\n",
    "        image_utz = image_header['UT-OBS']\n",
    "        object_name = image_header['OBJECT']\n",
    "        object_ra = str(image_header['RA'])\n",
    "        object_dec = str(image_header['DEC'])\n",
    "        object_alt =str(image_header['TELALT'])\n",
    "        object_az = str(image_header['TELAZ'])\n",
    "        camera_focus = str(image_header['CAMFOCUS'])\n",
    "        local_temp = str(image_header['TELTEMP'])\n",
    "        local_lat = str(image_header['LATITUDE'])\n",
    "        local_long = str(image_header['LONGITUD'])\n",
    "        local_weather = str(image_header['WEATHER'])\n",
    "\n",
    "    cursor = wxdconnection.cursor()\n",
    "\n",
    "    # I know this is a crime\n",
    "    sql = f'''\n",
    "        INSERT INTO iceberg_data.fits.\"fits-images\" (\n",
    "            filename, \n",
    "            filebytes,\n",
    "            image_width ,\n",
    "            image_height ,\n",
    "            image_utz ,\n",
    "            object_name ,\n",
    "            object_ra ,\n",
    "            object_dec ,\n",
    "            object_alt ,\n",
    "            object_az ,\n",
    "            camera_focus ,\n",
    "            local_temp ,\n",
    "            local_lat ,\n",
    "            local_long ,\n",
    "            local_weather \n",
    "        )\n",
    "        VALUES ( \n",
    "            '{image_file}',\n",
    "            '{encoded_file_content}',\n",
    "            '{image_width}' ,\n",
    "            '{image_height} ',\n",
    "            '{image_utz}' ,\n",
    "            '{object_name}' ,\n",
    "            '{object_ra} ',\n",
    "            '{object_dec}' ,\n",
    "            '{object_alt}' ,\n",
    "            '{object_az} ',\n",
    "            '{camera_focus}' ,\n",
    "            '{local_temp} ',\n",
    "            '{local_lat}' ,\n",
    "            '{local_long}' ,\n",
    "            '{local_weather}'                     \n",
    "        )\n",
    "    '''  \n",
    "\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        wxdconnection.commit() \n",
    "    except Exception as err:\n",
    "        print(f\"Error executing SQL: {repr(err)}\")\n",
    "    finally:\n",
    "        cursor.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell triggers the metadata extraction of the image files that build up the private image repository. This metadata will be inserted intoa table in watsonx.data using the Presto engine. \n",
    "As in the previous section, this initialization must be run once. A re-creation from scratch can be done at any time, but it is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wxdconnection = connect_to_watsonxdata()\n",
    "\n",
    "create_metadata_table(wxdconnection)\n",
    "\n",
    "file_paths = glob.glob(\"./images/m31*.FITS\")\n",
    "for image_file in sorted(file_paths):\n",
    "        print(\"Inserting file: \", image_file)\n",
    "        insert_file(wxdconnection,image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate picture events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section implements the role of the event generator described in the demo documentation. Essentially, it:\n",
    "\n",
    "- reads pictures captured by the astronomical camera (FITS files)\n",
    "- extracts the metadata\n",
    "- composes an event consisting of:\n",
    "  - the FITS file encoded as base64 to protect the content\n",
    "  - the metadata\n",
    "- optionally, creates a Kafka topic from scratch matching the event\n",
    "- sends the event as a message through the topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell implements a few functions. They will be called during the main workflow of the section. This cell will trigger no action. It is only definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import base64\n",
    "\n",
    "from astropy.io import fits\n",
    "from confluent_kafka import Producer, KafkaError\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "BROKER = 'watsonxdata:29092' \n",
    "topic = 'fits-images'  \n",
    "\n",
    "# Creates a new Kafka topic with the specified name, number of partitions\n",
    "# and replication factor. It first checks if the topic already exists by listing \n",
    "# all topics and checking if the given topic name is present. If the topic exists, \n",
    "# it deletes the topic and waits for it to be completely deleted before creating a new one.\n",
    "def create_kafka_topic(topic_name, num_partitions=1, replication_factor=1):\n",
    "\n",
    "    admin_client = AdminClient({'bootstrap.servers': BROKER})\n",
    "\n",
    "    existing_topics = admin_client.list_topics(timeout=10).topics\n",
    "\n",
    "    if topic_name in existing_topics:\n",
    "        delete_futures = admin_client.delete_topics([topic_name], operation_timeout=30)\n",
    "        for topic, future in delete_futures.items():\n",
    "            try:\n",
    "                future.result()  \n",
    "                print(f\"Topic '{topic}' has been marked for deletion.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete topic '{topic}': {e}\")\n",
    "\n",
    "        \n",
    "        start_time = time.time()\n",
    "        timeout = 10  # I will wait 10 seconds\n",
    "        check_interval = 5 # I will sleep 5 seconds until waiting again\n",
    "\n",
    "        while time.time() - start_time < timeout:\n",
    "            \n",
    "            metadata = admin_client.list_topics(timeout=timeout)\n",
    "            \n",
    "            if topic_name not in metadata.topics:\n",
    "                print(f\"Topic '{topic_name}' has been successfully deleted.\")\n",
    "                return\n",
    "            else:\n",
    "                print(f\"Topic '{topic_name}' is still being deleted. Checking again in {check_interval} seconds...\")\n",
    "\n",
    "            time.sleep(check_interval)\n",
    "\n",
    "    topic_list = [NewTopic(topic=topic_name, num_partitions=num_partitions, replication_factor=replication_factor)]\n",
    "\n",
    "    fs = admin_client.create_topics(new_topics=topic_list)\n",
    "\n",
    "    for topic, f in fs.items():\n",
    "        try:\n",
    "            f.result()  \n",
    "            print(f\"Created Kafka topic: {topic}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create topic {topic}: {e}\")\n",
    "\n",
    "# Creates a Kafka producer using the given configuration\n",
    "def create_kafka_producer():\n",
    "    conf = {\n",
    "        'bootstrap.servers': BROKER,\n",
    "        'client.id': 'fits_image_producer',\n",
    "    }\n",
    "    return Producer(conf)\n",
    "\n",
    "# Reads a FITS image file and returns its header plus the full content as a base64-encoded string.\n",
    "def read_fits_image_as_base64(fits_image_path) :\n",
    "\n",
    "    with open(fits_image_path, 'rb') as file :\n",
    "        file_content = file.read()\n",
    "    encoded_file_content = base64.b64encode(file_content).decode('utf-8')\n",
    "    with fits.open(fits_image_path) as hdul:\n",
    "        image_header = hdul[0].header\n",
    "\n",
    "    return image_header, encoded_file_content\n",
    "\n",
    "# Send a FITS image file and its metadata to a Kafka topic\n",
    "def send_file_image_to_kafka(producer, topic, file, image_base64, header):\n",
    "\n",
    "    image_width = str(header['NAXIS1'])\n",
    "    image_height = str(header['NAXIS2'])\n",
    "    image_utz = header['UT-OBS']\n",
    "    object_name = header['OBJECT']\n",
    "    object_ra = str(header['RA'])\n",
    "    object_dec = str(header['DEC'])\n",
    "    object_alt =str( header['TELALT'])\n",
    "    object_az = str(header['TELAZ'])\n",
    "    camera_focus = str(header['CAMFOCUS'])\n",
    "    local_temp = str(header['TELTEMP'])\n",
    "    local_lat = str(header['LATITUDE'])\n",
    "    local_long = str(header['LONGITUD'])\n",
    "    local_weather = str(header['WEATHER'])\n",
    "\n",
    "    event = {\n",
    "        'file':         file,\n",
    "        'image_width':  image_width,\n",
    "        'image_height': image_height,\n",
    "        'image_utz':    image_utz,\n",
    "        'object_name':  object_name,\n",
    "        'object_ra':    object_ra,\n",
    "        'object_dec':   object_dec,\n",
    "        'object_alt':   object_alt,\n",
    "        'object_az':    object_az,\n",
    "        'camera_focus': camera_focus,\n",
    "        'local_temp':   local_temp,\n",
    "        'local_lat':    local_lat,\n",
    "        'local_long':   local_long,\n",
    "        'local_weather':local_weather,   \n",
    "        'image_data':   image_base64        \n",
    "    }\n",
    "    \n",
    "    producer.produce(topic, key=\"fits_image\", value=json.dumps(event), callback=delivery_report)\n",
    "    producer.flush()\n",
    "\n",
    "# This function is called by the Kafka producer when it has received a \n",
    "# delivery report from the broker. It is used to print out information\n",
    "# about the message delivery status.\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Message delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this only the first time you run the demo or if you want to recreate the topic from scratch. If you run it, you may need to register the topic in watsonx.data again (upload the json file) if you want to access the virtual kafka table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_kafka_topic(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs the workflow of the event generation. It emulates the capture of two pictures and send them as Kafka messages. It can be run many times and you can change the files to be sent (`file_image_path`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "producer = create_kafka_producer()\n",
    "\n",
    "# This is a normal picture\n",
    "file_image_path = './images/m31.fits'  \n",
    "image_header, image_base64 = read_fits_image_as_base64(file_image_path)\n",
    "send_file_image_to_kafka(producer, topic, file_image_path, image_base64, image_header)\n",
    "\n",
    "print(f'File \"{file_image_path}\" sent to Kafka topic \"{topic}\".')\n",
    "\n",
    "# This is an anomaly\n",
    "file_image_path = './images/m31dot.fits'  \n",
    "image_header, image_base64 = read_fits_image_as_base64(file_image_path)\n",
    "send_file_image_to_kafka(producer, topic, file_image_path, image_base64, image_header)\n",
    "\n",
    "print(f'File \"{file_image_path}\" sent to Kafka topic \"{topic}\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Event reception and archival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import base64\n",
    "import prestodb\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "def create_kafka_consumer():\n",
    "    conf = {\n",
    "        'bootstrap.servers': 'watsonxdata:29092',  \n",
    "        'group.id': 'fits_image_group',\n",
    "        'auto.offset.reset': 'earliest'\n",
    "    }\n",
    "    return Consumer(conf)\n",
    "\n",
    "def save_base64_fits_image(base64_image, output_fits_path):\n",
    "    \n",
    "    image_bytes = base64.b64decode(base64_image)\n",
    "\n",
    "    with open(output_fits_path, 'wb') as f:\n",
    "        f.write(image_bytes)\n",
    "\n",
    "def connect_to_watsonxdata() :\n",
    "\n",
    "    userid     = 'ibmlhadmin'\n",
    "    password   = 'password'\n",
    "    hostname   = 'watsonxdata'\n",
    "    port       = '8443'\n",
    "    catalog    = 'tpch'\n",
    "    schema     = 'tiny'\n",
    "    certfile   = \"/certs/lh-ssl-ts.crt\"\n",
    "\n",
    "    try:\n",
    "        wxdconnection = prestodb.dbapi.connect(\n",
    "                host=hostname,\n",
    "                port=port,\n",
    "                user=userid,\n",
    "                catalog=catalog,\n",
    "                schema=schema,\n",
    "                http_scheme='https',\n",
    "                auth=prestodb.auth.BasicAuthentication(userid, password)\n",
    "        )\n",
    "        if (certfile != None):\n",
    "            wxdconnection._http_session.verify = certfile\n",
    "        print(\"Connection successful\")\n",
    "        return wxdconnection\n",
    "    except Exception as e:\n",
    "        print(\"Unable to connect to the database.\")\n",
    "        print(repr(e))\n",
    "\n",
    "\n",
    "def insert_into_watsonxdata(wxdconnection,\n",
    "                            file,\n",
    "                            image_width  ,\n",
    "                            image_height ,\n",
    "                            image_utz    ,\n",
    "                            object_name  ,\n",
    "                            object_ra    ,\n",
    "                            object_dec   ,\n",
    "                            object_alt   ,\n",
    "                            object_az    ,\n",
    "                            camera_focus ,\n",
    "                            local_temp   ,\n",
    "                            local_lat    ,\n",
    "                            local_long   ,\n",
    "                            local_weather,  \n",
    "                            image_base64\n",
    "                            ) :\n",
    "\n",
    "    cursor = wxdconnection.cursor()\n",
    "\n",
    "    sql = '''\n",
    "        CREATE SCHEMA IF NOT EXISTS \n",
    "            iceberg_data.angel \n",
    "        WITH (location = 's3a://iceberg-bucket/angel') \n",
    "    '''\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "    except Exception as err:\n",
    "        print(repr(err))\n",
    "\n",
    "    sql = '''\n",
    "        drop table if exists iceberg_data.angel.\"fits-images-from-message\"\n",
    "    '''\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "    except Exception as err:\n",
    "        print(repr(err))\n",
    "    \n",
    "    sql = '''\n",
    "        create table iceberg_data.angel.\"fits-images-from-message\" (     \n",
    "            file VARCHAR,\n",
    "            image_width VARCHAR,\n",
    "            image_height VARCHAR,\n",
    "            image_utz VARCHAR,\n",
    "            object_name VARCHAR,\n",
    "            object_ra VARCHAR,\n",
    "            object_dec VARCHAR,\n",
    "            object_alt VARCHAR,\n",
    "            object_az VARCHAR,\n",
    "            camera_focus VARCHAR,\n",
    "            local_temp VARCHAR,\n",
    "            local_lat VARCHAR,\n",
    "            local_long VARCHAR,\n",
    "            local_weather VARCHAR,\n",
    "            image_data VARCHAR\n",
    "        )\n",
    "    '''\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "    except Exception as err:\n",
    "        print(repr(err))\n",
    "\n",
    "\n",
    "    # I know this is a crime\n",
    "    sql = f'''\n",
    "        INSERT INTO iceberg_data.angel.\"fits-images-from-message\" (\n",
    "            file             , \n",
    "            image_width      ,\n",
    "            image_height     ,\n",
    "            image_utz        ,\n",
    "            object_name      ,\n",
    "            object_ra        ,\n",
    "            object_dec       ,\n",
    "            object_alt       ,\n",
    "            object_az        ,\n",
    "            camera_focus     ,\n",
    "            local_temp       ,\n",
    "            local_lat        ,\n",
    "            local_long       ,\n",
    "            local_weather    ,\n",
    "            image_data\n",
    "        )\n",
    "        VALUES ( \n",
    "            '{file}'          ,\n",
    "            '{image_width}'   ,\n",
    "            '{image_height}'  ,\n",
    "            '{image_utz}'     ,\n",
    "            '{object_name}'   ,\n",
    "            '{object_ra}'     ,\n",
    "            '{object_dec}'    ,\n",
    "            '{object_alt}'    ,\n",
    "            '{object_az}'     ,\n",
    "            '{camera_focus}'  ,\n",
    "            '{local_temp}'    ,\n",
    "            '{local_lat}'     ,\n",
    "            '{local_long}'    ,\n",
    "            '{local_weather}' ,\n",
    "            '{image_base64}'                       \n",
    "        )\n",
    "    \n",
    "    '''  \n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        wxdconnection.commit()\n",
    "    except Exception as err:\n",
    "        print(f\"Error executing SQL: {repr(err)}\")\n",
    "    finally:\n",
    "        cursor.close() \n",
    "\n",
    "    print(f'Inserted: {file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic = 'fits-images'   \n",
    "\n",
    "\n",
    "consumer = create_kafka_consumer()\n",
    "consumer.subscribe([topic])\n",
    "\n",
    "\n",
    "duration = 30\n",
    "start_time = time.time()\n",
    "\n",
    "print(f'Waiting \"{duration}\" seconds for messages on topic \"{topic}\"...')\n",
    "\n",
    "try:\n",
    "    while time.time() - start_time < duration:\n",
    "        msg = consumer.poll(1.0)  \n",
    "\n",
    "        if msg is None:\n",
    "            continue \n",
    "\n",
    "        if msg.error():           \n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                print(f\"Reached end of partition for topic {msg.topic()}, partition {msg.partition()}\")\n",
    "            elif msg.error():\n",
    "                print(f\"Error occurred: {msg.error()}\")\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            event = json.loads(msg.value().decode('utf-8'))\n",
    "            \n",
    "            file         = event.get('file')\n",
    "            image_width  = event.get('image_width')\n",
    "            image_height = event.get('image_height')\n",
    "            image_utz    = event.get('image_utz')\n",
    "            object_name  = event.get('object_name')\n",
    "            object_ra    = event.get('object_ra')\n",
    "            object_dec   = event.get('object_dec')\n",
    "            object_alt   = event.get('object_alt')\n",
    "            object_az    = event.get('object_az')\n",
    "            camera_focus = event.get('camera_focus')\n",
    "            local_temp   = event.get('local_temp')\n",
    "            local_lat    = event.get('local_lat')\n",
    "            local_long   = event.get('local_long')\n",
    "            local_weather= event.get('local_weather')\n",
    "            image_base64 = event.get('image_data')\n",
    "            \n",
    "            print(f'Received message: {file}')\n",
    "\n",
    "            wxdconnection = connect_to_watsonxdata()\n",
    "            insert_into_watsonxdata(wxdconnection, \n",
    "                                    file,\n",
    "                                    image_width  ,\n",
    "                                    image_height ,\n",
    "                                    image_utz    ,\n",
    "                                    object_name  ,\n",
    "                                    object_ra    ,\n",
    "                                    object_dec   ,\n",
    "                                    object_alt   ,\n",
    "                                    object_az    ,\n",
    "                                    camera_focus ,\n",
    "                                    local_temp   ,\n",
    "                                    local_lat    ,\n",
    "                                    local_long   ,\n",
    "                                    local_weather,  \n",
    "                                    image_base64)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Shutting down consumer...\")\n",
    "\n",
    "finally:\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Read image from watsonx.data and search Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "import prestodb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from astropy.io import fits\n",
    "from skimage.transform import resize\n",
    "\n",
    "from pymilvus import(\n",
    "    Milvus,\n",
    "    IndexType,\n",
    "    Status,\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    utility,\n",
    "    MilvusClient\n",
    ")\n",
    "\n",
    "\n",
    "def connect_to_milvus() :\n",
    "\n",
    "    # This is for Baklarz's image\n",
    "    host         = 'watsonxdata'\n",
    "    port         = 19530\n",
    "    user         = 'ibmlhadmin'\n",
    "    key          = 'password'\n",
    "    server_pem_path = 'presto.crt'\n",
    "    connections.connect(alias='default',\n",
    "                       host=host,\n",
    "                       port=port,\n",
    "                       user=user,\n",
    "                       password=key,\n",
    "                       server_pem_path=server_pem_path,\n",
    "                       server_name='watsonxdata',\n",
    "                       secure=True)  \n",
    "\n",
    "    # This is for SaaS\n",
    "    # host         = 'acb3dba1-2c32-4c99-9833-6d060a2e32b4.cqh2jh8d00ae3kp0jmpg.lakehouse.appdomain.cloud'\n",
    "    # port         = 30969\n",
    "    # user         = 'ibmlhapikey'\n",
    "    # key          = 'Xndw8q4VKrLoqM2SB_zwbEuqfyH-9d2zwCyaKFIsEElF'\n",
    "    # connections.connect(         \n",
    "    #     host=host, \n",
    "    #     port=port,\n",
    "    #     user=user,\n",
    "    #     password=key,\n",
    "    #     secure=True,\n",
    "    # )\n",
    "    \n",
    "    print(f\"\\nList connections:\")\n",
    "    print(connections.list_connections())\n",
    "\n",
    "    \n",
    "def load_fits_file(file_path) :\n",
    "    \n",
    "    with fits.open(file_path) as hdul:\n",
    "   \n",
    "        image_data = hdul[0].data\n",
    "        image_resized = resize(image_data, (166, 100), mode='reflect')\n",
    "\n",
    "    return (image_resized ) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_embedding(image_data) : \n",
    "    \n",
    "    embedding = image_data.flatten()\n",
    "    embedding = embedding / np.linalg.norm(embedding)  # Normalizing the embedding\n",
    "    \n",
    "    return embedding\n",
    "    \n",
    "def search_image(search_collection, kafka_data) :\n",
    "\n",
    "    file_contents = base64.b64decode(kafka_data)\n",
    "    fits_file = io.BytesIO(file_contents)\n",
    "    with fits.open(fits_file) as hdul:\n",
    "        # hdul.info()\n",
    "        image_data = hdul[0].data\n",
    "\n",
    "    image_resized = resize(image_data, (166, 100), mode='reflect')\n",
    "\n",
    "    embedding_vector = generate_embedding(image_resized)\n",
    "    query_embedding = [embedding_vector]\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 1000}}\n",
    "    search_collection.load()\n",
    "    results = search_collection.search(\n",
    "        data=query_embedding,\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        limit=3,\n",
    "        output_fields=[\"id\", \"file_path\"],  \n",
    "        expr=None\n",
    "    )\n",
    "    # Here, metric_type is set to \"L2\", which refers to the Euclidean distance metric. \n",
    "    # This means that the search will calculate the L2 distance (or squared \n",
    "    # Euclidean distance) between the query vector and the stored vectors in the collection. \n",
    "\n",
    "    for result in results[0]:\n",
    "        print(f\"Image File: {result.file_path}, Difference: {result.distance:.2%}\")\n",
    "\n",
    "\n",
    "def connect_to_watsonxdata() :\n",
    "\n",
    "    import warnings\n",
    "\n",
    "    # Suppress the specific UserWarning from pandas\n",
    "    warnings.filterwarnings(\"ignore\", message=\"pandas only supports SQLAlchemy connectable\")\n",
    "\n",
    "    # Connection Parameters\n",
    "    userid     = 'ibmlhadmin'\n",
    "    password   = 'password'\n",
    "    hostname   = 'watsonxdata'\n",
    "    port       = '8443'\n",
    "    catalog    = 'tpch'\n",
    "    schema     = 'tiny'\n",
    "    certfile   = \"/certs/lh-ssl-ts.crt\"\n",
    "\n",
    "    # Connect Statement\n",
    "    try:\n",
    "        wxdconnection = prestodb.dbapi.connect(\n",
    "                host=hostname,\n",
    "                port=port,\n",
    "                user=userid,\n",
    "                catalog=catalog,\n",
    "                schema=schema,\n",
    "                http_scheme='https',\n",
    "                auth=prestodb.auth.BasicAuthentication(userid, password)\n",
    "        )\n",
    "        if (certfile != None):\n",
    "            wxdconnection._http_session.verify = certfile\n",
    "        cursor = wxdconnection.cursor()\n",
    "        print(\"Connection successful\")\n",
    "        return wxdconnection\n",
    "    except Exception as e:\n",
    "        print(\"Unable to connect to the database.\")\n",
    "        print(repr(e))\n",
    "\n",
    "def get_images_from_watsonxdata(wxdconnection) :\n",
    "\n",
    "    sql = '''\n",
    "    SELECT json_extract_scalar(_message, '$.file') AS \"file\",\n",
    "           json_extract_scalar(_message, '$.image_data') AS \"image_data\"\n",
    "    FROM \"kafka\".\"default\".\"fits-images\" \n",
    "    LIMIT 100 \n",
    "    '''\n",
    "    try:\n",
    "        df = pd.read_sql(sql,wxdconnection)\n",
    "        if (len(df) == 0):\n",
    "            print(\"No rows found.\")\n",
    "    except Exception as e:\n",
    "        print(repr(e))\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_to_milvus()\n",
    "fits_coll = Collection(\"image_embeddings\")\n",
    "\n",
    "wxdconnection = connect_to_watsonxdata()\n",
    "data_images = get_images_from_watsonxdata(wxdconnection)\n",
    "\n",
    "for index, row in data_images.iterrows():\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Comparing the file:\", row['file'])\n",
    "    search_image(fits_coll,row['image_data'])\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "connections.disconnect(alias=\"default\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
